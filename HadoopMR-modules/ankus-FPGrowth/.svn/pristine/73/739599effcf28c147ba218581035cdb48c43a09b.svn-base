package org.ankus.mapreduce.algorithms.association.pfpgrowth;

import java.io.IOException;
import java.util.StringTokenizer;

import org.ankus.util.ArgumentsConstants;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

//import PFP_growth.Pfp_param;

public class PfpgrowthSupportCountMapper extends Mapper<LongWritable , Text, Text, IntWritable>{
	private Logger logger = LoggerFactory.getLogger(PfpgrowthSupportCountMapper.class);
	private final static IntWritable one = new IntWritable(1);
	private Text item = new Text();
	
	public void setup(Context context) throws IOException, InterruptedException
	{
	
	}
	public void map(LongWritable key, Text value, Context context) 
			throws IOException, InterruptedException
	{
		String delimiter =  context.getConfiguration().get(ArgumentsConstants.DELIMITER, "\t");
		StringTokenizer itr = new StringTokenizer(value.toString(), delimiter);
//		int data_idx = 0;
//		int data_start = 4;
//		while(itr.hasMoreTokens())
//		{
//			String token = itr.nextToken();
//			if(data_idx >= data_start)
//			{
//				item.set(token.split(":")[0]);	
//				context.write(item,one);
//			}			
//			data_idx++;
//		}	
		
		while(itr.hasMoreTokens())
		{
			String token = itr.nextToken();
			item.set(token);	
			context.write(item,one);
	
		}	
		Counter counter = context.getCounter("Pfpgrowth","TRANSACTIONS");
        counter.increment(1);
	}
	@Override
	protected void cleanup(Context context) throws IOException, InterruptedException
	{
		
	}
}