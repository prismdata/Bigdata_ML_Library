package org.ankus.mapreduce.algorithms.clustering.FuzzyCMeans;

import org.ankus.util.ArgumentsConstants;
import org.ankus.util.ConfigurationVariable;
import org.ankus.util.Constants;
import org.ankus.util.Usage;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class WeightSearchDriver{
	private Logger logger = LoggerFactory.getLogger(WeightSearchDriver.class);
	public String instance_hash = "";
	public String cluster_weight = "";
	public int iteration_count = 0;
	public String input_path ="";
	
	public int run()  throws Exception 
	{
		Configuration conf = new Configuration();
	    Job job = Job.getInstance(conf, "word count");
	    job.setJarByClass(WeightSearchDriver.class);
	    job.setMapperClass(Map_WeightSearch.class);
	    job.setOutputKeyClass(Text.class);
	    job.setOutputValueClass(Text.class);
	    FileInputFormat.addInputPath(job, new Path(input_path));
	    FileOutputFormat.setOutputPath(job, new Path(input_path+"_srch"));
	    if(!job.waitForCompletion(true))
        {
            logger.info("Error: Get Terms is not Completeion");
            return 1;
        }
	    return 0;
	}
	public void main(String args[]) throws Exception 
	{
		
	}
}
